<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning on Rabin Adhikari</title><link>https://rabinadhikari.com.np/categories/deep-learning/</link><description>Recent content in Deep Learning on Rabin Adhikari</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Fri, 07 May 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://rabinadhikari.com.np/categories/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Training of Word Embeddings</title><link>https://rabinadhikari.com.np/posts/training-word-embeddings/</link><pubDate>Fri, 07 May 2021 00:00:00 +0000</pubDate><guid>https://rabinadhikari.com.np/posts/training-word-embeddings/</guid><description>Tokenization Since the computer doesnâ€™t understand strings as a numerical value, they need to be converted into numeric forms. So the sentences used for training are converted into sequence tokens using some kind of tokenizer. The process of converting a text into a sequence of tokens is called tokenization. Tokenization can be broadly classified into 3 types viz. word, character, and subword (n-gram characters).
Word Tokenizer Word tokenizers tokenize each word separately by splitting the sentence with spaces.</description></item></channel></rss>